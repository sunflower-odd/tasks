# Duplicate Detection Tool for Product Catalogs

## Описание

Вариант 3 из экзаменационного задания по дисциплине **Алгоритмы и структуры данных**

Эта программа позволяет находить потенциальные дубликаты товаров в каталоге, сравнивая новые товары с уже существующими. 
Алгоритм использует **расстояние Левенштейна** для оценки схожести строк, а также базовую предобработку текста (нормализация, удаление лишних символов, обработка синонимов цветов и исключение служебных слов).
Программа работает полностью на стандартных библиотеках Python и не использует сторонние библиотеки для сравнения строк.  

---

## Структура проекта

- **main.py** — основной скрипт с функциями для предобработки данных, токенизации, поиска похожих товаров и записи результатов.  
- **config.py** — конфигурационный файл с настройками:
  - `USE_PREPROCESSING` — включение/выключение предобработки.
  - `COLOR_MAP` — словарь синонимов цветов.
  - `WORDS_TO_EXCLUDE` — список служебных слов для исключения.
  - `SIMILARITY_THRESHOLD` — порог схожести для определения дубликата (0–1).  
  - `CATALOG_FILE_NAME` — путь к файлу с каталогом товаров.  
  - `NEW_ITEMS_FILE_NAME` — путь к файлу с новыми товарами, которые мы проверяем на дубликаты, сравнивая с существующим каталогом.

- **duplicates.json** — выходной файл с результатами поиска дубликатов.

---

## Основные функции и их логика

### 1. `_levenshtein_distance(a, b)`

- **Принимает:** две строки (`a`, `b`).  
- **Делает:** вычисляет расстояние Левенштейна — минимальное количество операций (вставка, удаление, замена), чтобы превратить одну строку в другую.  
- **Возвращает:** целое число — расстояние между строками.  

### 2. `clean_data(file_name)`

- **Принимает:** путь к текстовому файлу с товарами.  
- **Делает:**
  - Если включена предобработка:
    - Перевод текста в нижний регистр.  
    - Удаление всех символов кроме букв, цифр и пробелов.  
    - Объединение нескольких пробелов в один.  
    - Нормализация единиц памяти (`ГБ` → `gb`).  
    - Разделение строки на токены (слова и числа).  
    - Замена синонимов цветов через `COLOR_MAP`.  
    - Исключение служебных слов (`WORDS_TO_EXCLUDE`).  
  - Если предобработка отключена — просто разделяет строку на токены.  
- **Возвращает:** словарь вида `{item_id: [список токенов]}`.

### 3. `tokenize(cleaned_dict)`

- **Принимает:** словарь после очистки (`cleaned_dict`).  
- **Делает:** разделяет токены на числовые и текстовые, сортирует их.  
- **Возвращает:** структурированный словарь:
```python
{
    "item_id": {
        "numbers": ["128", "64"],
        "words": ["black", "phone"]
    },
    ...
}
```

### 4. `main(new_items_file_name, catalog_file_name)` 
- реализует полную логику пайплайна